{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process anitSMASH outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, rename, path\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up folder names from antiSMASH raw output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antismash_results = \"../antismash_results\"\n",
    "\n",
    "for assembly in listdir(antismash_results):\n",
    "    if assembly == \".DS_Store\": # MacOS thing\n",
    "        continue\n",
    "    if assembly.endswith(\"_assembly\"):\n",
    "        new_name = assembly.replace(\"_assembly\", \"\")\n",
    "        current_path = path.join(antismash_results, assembly)\n",
    "        new_path = path.join(antismash_results, new_name)\n",
    "\n",
    "        if path.exists(new_path):\n",
    "            continue\n",
    "        else:\n",
    "            rename(current_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify filenames to avoid duplicate names when running BiG-SCAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antismash_results = \"../antismash_results\"\n",
    "\n",
    "for assembly in listdir(antismash_results):\n",
    "    assembly_path = f\"{antismash_results}/{assembly}\"\n",
    "    if assembly in [\".DS_Store\"]: # MacOS thing\n",
    "        continue\n",
    "\n",
    "    for file in listdir(assembly_path):\n",
    "        if \".region\" in file:\n",
    "            new_name = f\"{assembly}_{file}\"\n",
    "            current_path = path.join(assembly_path, file)\n",
    "            new_path = path.join(assembly_path, new_name)\n",
    "            \n",
    "            if path.exists(new_path):\n",
    "                continue\n",
    "            else:\n",
    "                rename(current_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all BGC annotations from antiSMASH region GenBank files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bgc_annos(assembly_path: str, assembly: str):\n",
    "    \"\"\" Extract annotations from each BGC predicted by antiSMASH \"\"\"\n",
    "\n",
    "    all_annos = []  # Master list to store annotations from all GenBank files\n",
    "\n",
    "    for file in listdir(assembly_path):\n",
    "        if \".region\" in file:\n",
    "            file_path = f\"{assembly_path}/{file}\"\n",
    "            \n",
    "            with open(file_path, \"r\") as gbk:\n",
    "                records = SeqIO.parse(gbk, \"genbank\")\n",
    "\n",
    "                for record in records:\n",
    "                    for feature in record.features:\n",
    "                        if feature.type == \"protocluster\":\n",
    "                            tmp_dict = {}\n",
    "                            tmp_dict[\"Position\"] = file.split(\".gbk\")[0]\n",
    "\n",
    "                            number = feature.qualifiers.get(\"protocluster_number\").pop()\n",
    "                            category = feature.qualifiers.get(\"category\").pop()\n",
    "                            product = feature.qualifiers.get(\"product\").pop()\n",
    "\n",
    "                            tmp_dict[\"Protocluster\"] = number\n",
    "                            tmp_dict[\"Category\"] = category\n",
    "                            tmp_dict[\"Product\"] = product\n",
    "                            \n",
    "                            references = []\n",
    "                            for q in feature.qualifiers:\n",
    "                                if q.endswith(\"product_classes\"):\n",
    "                                    references = feature.qualifiers.get(q, [])  # Get reference(s) if found\n",
    "                            \n",
    "                            if not references:\n",
    "                                references = [product]  # Default to product if no reference found\n",
    "\n",
    "                            for ref in references:\n",
    "                                new_entry = tmp_dict.copy()\n",
    "                                new_entry[\"Reference\"] = ref\n",
    "                                new_entry[\"Assembly\"] = assembly\n",
    "                                all_annos.append(new_entry)\n",
    "\n",
    "    if all_annos:\n",
    "        df = pd.DataFrame(all_annos)\n",
    "\n",
    "        column_order = [\"Assembly\", \"Position\", \"Protocluster\", \"Category\", \"Product\", \"Reference\"]\n",
    "        df = df[column_order]\n",
    "\n",
    "        # Save individual assembly BGC annotations\n",
    "        output_path = f\"{assembly_path}/{assembly}_bgc_annotations.csv\"\n",
    "        df.to_csv(output_path, index=False)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    return None\n",
    "\n",
    "def process_all_folders(antismash_results_path: str):\n",
    "    \"\"\"Loops through all subdirectories in 'antismash_results' and processes them.\"\"\"\n",
    "    all_dfs = []  # List to collect all individual DataFrames\n",
    "\n",
    "    for assembly in listdir(antismash_results_path):\n",
    "        if assembly == \".DS_Store\":  # Skip system files (MacOS thing)\n",
    "            continue\n",
    "        \n",
    "        assembly_path = f\"{antismash_results_path}/{assembly}\"\n",
    "        df = extract_bgc_annos(assembly_path=assembly_path, assembly=assembly)\n",
    "\n",
    "        if df is not None:\n",
    "            all_dfs.append(df)\n",
    "\n",
    "    # Merge all DataFrames into one\n",
    "    if all_dfs:\n",
    "        merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        merged_df.to_csv(f\"{antismash_results_path}/all_bgc_annotations_from_region_gbk.csv\", index=False)\n",
    "        return merged_df\n",
    "\n",
    "    return \"No BGC annotations found in any assembly!\"\n",
    "\n",
    "# Run the processing\n",
    "antismash_results = \"../antismash_results\"\n",
    "merged_annotations = process_all_folders(antismash_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract BGC annotations from the HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bgcs_from_html(assembly_path: str, assembly: str):\n",
    "    \"\"\" Extract more specific BGC annotations from antiSMASH index.html output file \"\"\"\n",
    "\n",
    "    data = {\"SEQID\": [], \"Product\": []}\n",
    "\n",
    "    html_path = f\"{assembly_path}/index.html\"\n",
    "    with open (html_path) as fp:\n",
    "        soup = BeautifulSoup(fp, 'html.parser')\n",
    "\n",
    "    unique_links = set()\n",
    "\n",
    "    for td in soup.find_all(\"td\"):\n",
    "        a_tags = td.find_all(\"a\", class_=\"external-link\")\n",
    "\n",
    "        if len(a_tags) == 1 and a_tags[0].get(\"target\") == \"_blank\":\n",
    "            text = a_tags[0].text.strip()\n",
    "            if text not in unique_links:\n",
    "                unique_links.add(text)\n",
    "    \n",
    "    if unique_links: \n",
    "        for i in unique_links:\n",
    "            data[\"Product\"].append(i)\n",
    "        data[\"SEQID\"] = [assembly] * len(unique_links)\n",
    "\n",
    "    else:\n",
    "        data[\"Product\"].append(\"Unknown bioactive compound\")\n",
    "        data[\"SEQID\"].append(assembly)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_all_html_bgcs(results_folder_path):\n",
    "    master = []\n",
    "    for assembly in listdir(results_folder_path):\n",
    "        if assembly != \".DS_Store\": # MacOS thing\n",
    "            result = get_bgcs_from_html(assembly_path = f\"{results_folder_path}/{assembly}\", assembly=assembly)\n",
    "            master.append(result)\n",
    "\n",
    "    master_df = pd.DataFrame(master)\n",
    "    master_df = master_df.explode([\"SEQID\", \"Product\"])\n",
    "\n",
    "    pd.DataFrame.to_csv(master_df, \"../antismash_results/all_bgc_annotations_from_html.csv\", index=False)\n",
    "    \n",
    "    return master_df\n",
    "\n",
    "antismash_results = \"../antismash_results\"\n",
    "\n",
    "get_all_html_bgcs(antismash_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract BGC annotations from JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_bgcs_from_json(assembly, assembly_path):\n",
    "    df = {\n",
    "        \"SEQID\": [],\n",
    "        \"Contig\": [],\n",
    "        \"Region\": [],\n",
    "        \"Category\": [],\n",
    "        \"Number of hits\": [],\n",
    "        \"Most similar known cluster\": [],\n",
    "        \"Similarity\": []\n",
    "    }\n",
    "\n",
    "    for file in listdir(assembly_path):\n",
    "        if file.endswith(\"assembly.json\"):\n",
    "            file_path = f\"{assembly_path}/{file}\"\n",
    "            with open(file_path) as f:\n",
    "                data = json.load(f)\n",
    "                records = data.get(\"records\")\n",
    "\n",
    "                for i in range(len(records)):\n",
    "                    record = records[i]\n",
    "                    contig = record[\"id\"]\n",
    "                    module = record[\"modules\"]\n",
    "\n",
    "                    for contig_region in record[\"features\"]:\n",
    "                        if contig_region[\"type\"] == \"region\":\n",
    "                            qualifier = contig_region[\"qualifiers\"]\n",
    "                            region = qualifier[\"region_number\"]\n",
    "                            p = qualifier[\"product\"]\n",
    "                            df[\"SEQID\"].append(assembly)    # Get SEQID\n",
    "                            df[\"Contig\"].append(contig)     # Get BGC contig number\n",
    "                            df[\"Region\"].append(region[0])  # Get BGC region number\n",
    "                            df[\"Category\"].append(p[0])     # Get BGC category\n",
    "\n",
    "                    for modkey in module.keys():\n",
    "\n",
    "                        # Getting ClusterBlast hits (all BGC-like regions):\n",
    "                        if modkey == \"antismash.modules.clusterblast\":\n",
    "                            line = module[modkey]\n",
    "\n",
    "                            c = line[\"knowncluster\"]\n",
    "                            c_results = c[\"results\"]\n",
    "\n",
    "                            for region_result in c_results:\n",
    "                                ranks = region_result[\"ranking\"]\n",
    "                                n_hits = region_result[\"total_hits\"]\n",
    "                                df[\"Number of hits\"].append(n_hits)\n",
    "\n",
    "                                bgc_list = []\n",
    "                                similarity_list = []\n",
    "\n",
    "                                for l in ranks:\n",
    "                                    for hits_dict in l:\n",
    "                                        if \"description\" in hits_dict:\n",
    "                                            bgc = hits_dict[\"description\"]\n",
    "                                            bgc_list.append(bgc)\n",
    "\n",
    "                                        if \"similarity\" in hits_dict:\n",
    "                                            similarity = hits_dict[\"similarity\"]/100\n",
    "                                            similarity_list.append(similarity)\n",
    "\n",
    "                                refs_dict = {bgc_list[i]: similarity_list[i] for i in range(len(bgc_list))}\n",
    "                                if len(refs_dict):\n",
    "                                    best_bgc = max(refs_dict, key=refs_dict.get)\n",
    "                                    best_bgc_similarity = refs_dict[best_bgc]\n",
    "\n",
    "                                    df[\"Most similar known cluster\"].append(best_bgc)\n",
    "                                    df[\"Similarity\"].append(best_bgc_similarity)\n",
    "                                \n",
    "                                if not len(refs_dict):\n",
    "                                    df[\"Most similar known cluster\"].append(None)\n",
    "                                    df[\"Similarity\"].append(0)\n",
    "\n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "def get_all_json_bgcs(antismash_path):\n",
    "    master_df = []\n",
    "\n",
    "    for assembly in listdir(antismash_path):\n",
    "        if assembly != \".DS_Store\": # MacOS thing\n",
    "            assembly_path = f\"{antismash_path}/{assembly}\"\n",
    "            df = get_bgcs_from_json(assembly, assembly_path)\n",
    "            master_df.append(df)\n",
    "\n",
    "    master_df = pd.concat(master_df, ignore_index=True)\n",
    "\n",
    "    # Replace NAs for missing most similar known clusters with category\n",
    "    master_df[\"Most similar known cluster\"] = master_df[\"Most similar known cluster\"].fillna(master_df[\"Category\"])\n",
    "\n",
    "    return pd.DataFrame.to_csv(master_df, \"../antismash_results/all_bgc_annotations_from_json.csv\", index=False)\n",
    "\n",
    "get_all_json_bgcs(\"../antismash_results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bakta results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract gene annotations from embl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wga_to_csv(input_file, output_file, file_format=\"embl\"):\n",
    "    \"\"\"Converts a whole-genome annotation in .gbk or .embl format to a CSV with gene/product info per contig and totals.\"\"\"\n",
    "\n",
    "    with open(input_file, \"r\") as infile, open(output_file, \"w\", newline=\"\") as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        headers = [\"SEQID\", \"Gene\", \"Product\"]\n",
    "        writer.writerow(headers)\n",
    "\n",
    "        global_totals = defaultdict(int)\n",
    "\n",
    "        for record in SeqIO.parse(infile, file_format):\n",
    "            seqid = record.id\n",
    "\n",
    "            for feature in record.features:\n",
    "                if feature.type == \"CDS\":\n",
    "                    gene = feature.qualifiers.get(\"gene\", [\"\"])[0]\n",
    "                    product = feature.qualifiers.get(\"product\", [\"\"])[0]\n",
    "\n",
    "                    writer.writerow([seqid, gene, product])\n",
    "\n",
    "                    # Optional: keep totals if needed\n",
    "                    global_totals[\"total_genes\"] += 1\n",
    "                    if gene:\n",
    "                        global_totals[gene] += 1\n",
    "\n",
    "# Run on bakta output from all assemblies\n",
    "for assembly in listdir(\"../bakta_results/\"):\n",
    "    embl_file = f\"../bakta_results/{assembly}/{assembly}_assembly.embl\"\n",
    "    if embl_file == \"../bakta_results/.DS_Store/.DS_Store.embl\": # MacOS thing\n",
    "        continue\n",
    "    wga_to_csv(embl_file, f\"../bakta_results/{assembly}/{assembly}_bakta.csv\", file_format=\"embl\")\n",
    "\n",
    "# Merge all bakta annotations into a single csv file\n",
    "master = []\n",
    "for assembly in listdir(\"../bakta_results\"):\n",
    "    assembly_path = f\"../bakta_results/{assembly}\"\n",
    "    if assembly == \".DS_Store\":\n",
    "        continue\n",
    "    \n",
    "    for file in listdir(assembly_path):\n",
    "        if file.endswith(\"bakta.csv\"):\n",
    "            bakta_csv_path = f\"{assembly_path}/{file}\"\n",
    "            bakta_df = pd.read_csv(bakta_csv_path)\n",
    "            bakta_df[\"SEQID\"] = assembly\n",
    "            master.extend(bakta_df[[\"SEQID\", \"Gene\", \"Product\"]].to_dict(orient=\"records\"))\n",
    "\n",
    "master_df = pd.DataFrame(master)\n",
    "\n",
    "pd.DataFrame.to_csv(master_df, \"../bakta_results/all_bakta_annos.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
